{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067be436-38be-499b-afd9-1e004ac7a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING = 0\n",
    "CRACK = 1\n",
    "LCD = 2\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11486602-e42f-4b3c-b735-da7cfa4085db",
   "metadata": {},
   "source": [
    "# Creating dummy data\n",
    "\n",
    "Only run this section if you want to test stuff quickly without dealing with the real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf19c2-f476-4d26-8178-5ba5358a8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "X = rng.integers(low = 0, high = 256, size = (100, 400, 300, 3))\n",
    "y = rng.integers(low = 0, high = NUM_CLASSES, size = 100).reshape((100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b8ab8-d7d8-4220-b10a-5d7facd33d64",
   "metadata": {},
   "source": [
    "# Reading data from files\n",
    "\n",
    "Don't run this section if you want to work with dummy data, it will overwrite variables `X` and `y` from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabe1c2-6e34-4239-acff-158e03156c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def collect_file_paths(path: str, ext: str) -> list[str]:\n",
    "    result = []\n",
    "    \n",
    "    for item in os.listdir(path):\n",
    "        item_path = os.path.join(path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            result.extend(collect_file_paths(item_path, ext))\n",
    "        elif item_path.endswith(ext):\n",
    "            result.append(item_path)\n",
    "                \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cee07c-dcf1-4a7b-ab45-393a0fa77a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def images_to_array(image_paths: list[str], shrink_factor: int) -> np.ndarray:\n",
    "    images = []\n",
    "    \n",
    "    for image_file in image_paths:\n",
    "        image = cv2.cvtColor(cv2.imread(image_file), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        new_size = (\n",
    "            image.shape[0] // shrink_factor, \n",
    "            image.shape[1] // shrink_factor\n",
    "        )\n",
    "        image = cv2.resize(image, dsize = new_size, interpolation = cv2.INTER_CUBIC)\n",
    "        images.append(image)\n",
    "        \n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec74d02-bb8f-4ea8-aeca-5b10129f2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = collect_file_paths(\"images_for_model\", \".jpeg\")\n",
    "\n",
    "input_filepaths = []\n",
    "labels_list = []\n",
    "\n",
    "for path in file_paths:\n",
    "    file_name = path.split(\"/\")[-1]\n",
    "    labels = file_name.split(\"_\")\n",
    "    \n",
    "    if not (\"unknown\" in labels):\n",
    "        if \"crack\" in labels:\n",
    "            labels_list.append(CRACK)\n",
    "        elif \"lcd\" in labels:\n",
    "            labels_list.append(LCD)\n",
    "        else:\n",
    "            labels_list.append(WORKING)\n",
    "            \n",
    "        input_filepaths.append(path)\n",
    "        \n",
    "X = images_to_array(input_filepaths, 20)\n",
    "y = np.array(labels_list).reshape((len(labels_list), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef85f8-6fb3-4dc8-bbad-6f989590a619",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f7f64-4325-4b7a-ab1a-2cd179dae55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = X.shape[0]\n",
    "IMAGE_WIDTH = X.shape[1]\n",
    "IMAGE_HEIGHT = X.shape[2]\n",
    "IMAGE_SHAPE = (X.shape[1], X.shape[2], X.shape[3])\n",
    "\n",
    "print(\"Entire dataset\")\n",
    "print(\"--------------\")\n",
    "print(\"Input shape: \", X.shape)\n",
    "print(\"Output shape: \", y.shape)\n",
    "print(\"Number of cracked phones: \", np.sum(y == CRACK))\n",
    "print(\"Number of phones with damaged lcd: \", np.sum(y == LCD))\n",
    "print(\"Number of working phones: \", np.sum(y == WORKING))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183e095-026b-4e7f-969e-e37ccb587108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: augment LCD images\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Training data\")\n",
    "print(\"-------------\")\n",
    "print(\"Number of cracked phones: \", np.sum(y_train == CRACK))\n",
    "print(\"Number of phones with damaged lcd: \", np.sum(y_train == LCD))\n",
    "print(\"Number of working phones: \", np.sum(y_train == WORKING))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Testing data\")\n",
    "print(\"-------------\")\n",
    "print(\"Number of cracked phones: \", np.sum(y_test == CRACK))\n",
    "print(\"Number of phones with damaged lcd: \", np.sum(y_test == LCD))\n",
    "print(\"Number of working phones: \", np.sum(y_test == WORKING))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b852d6-e9c1-49ea-a568-d2b0d52590b2",
   "metadata": {},
   "source": [
    "# Defining the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1b04e-19d4-48c7-89ce-fbb14bb8075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model, regularizers, optimizers, Sequential\n",
    "from keras.layers import Conv2D, Activation, Flatten, Dense, MaxPooling2D, BatchNormalization, Dropout\n",
    "\n",
    "# This Convolutional neural network architecture is taken from\n",
    "# Zeiler et al., Visualizing and Understanding Convolutional Networks (2013)\n",
    "def define_model(learning_rate = 0.001):\n",
    "    inputs = Input(shape = IMAGE_SHAPE)\n",
    "    \n",
    "    hidden = Conv2D(\n",
    "        filters = 96, \n",
    "        kernel_size = (7, 7), \n",
    "        strides = (2, 2),\n",
    "        activation = \"relu\"\n",
    "    )(inputs)\n",
    "    \n",
    "    hidden = MaxPooling2D(\n",
    "        pool_size = (3, 3), \n",
    "        strides = (2, 2)\n",
    "    )(hidden)\n",
    "    \n",
    "    # Instance (or Contrast) Normalization\n",
    "    # https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8\n",
    "    # https://stackoverflow.com/questions/68088889/how-to-add-instancenormalization-on-tensorflow-keras\n",
    "    hidden = BatchNormalization(\n",
    "        axis = 3\n",
    "    )(hidden, training = True)\n",
    "\n",
    "    hidden = Conv2D(\n",
    "        filters = 256, \n",
    "        kernel_size = (5, 5), \n",
    "        strides = (2, 2),\n",
    "        activation = \"relu\"\n",
    "    )(hidden)\n",
    "\n",
    "    hidden = MaxPooling2D(\n",
    "        pool_size = (3, 3), \n",
    "        strides = (2, 2)\n",
    "    )(hidden)\n",
    "    \n",
    "    hidden = BatchNormalization(\n",
    "        axis = 3\n",
    "    )(hidden, training = True)\n",
    "  \n",
    "    hidden = Conv2D(\n",
    "        filters = 384, \n",
    "        kernel_size = (3, 3), \n",
    "        strides = (1, 1),\n",
    "        padding = \"same\",\n",
    "        activation = \"relu\"\n",
    "    )(hidden)\n",
    "    \n",
    "    hidden = Conv2D(\n",
    "        filters = 384, \n",
    "        kernel_size = (3, 3), \n",
    "        strides = (1, 1),\n",
    "        padding = \"same\",\n",
    "        activation = \"relu\"\n",
    "    )(hidden)\n",
    "    \n",
    "    hidden = Conv2D(\n",
    "        filters = 256, \n",
    "        kernel_size = (3, 3), \n",
    "        strides = (1, 1),\n",
    "        padding = \"same\",\n",
    "        activation = \"relu\"\n",
    "    )(hidden)\n",
    "    \n",
    "    hidden = MaxPooling2D(\n",
    "        pool_size = (3, 3), \n",
    "        strides = (2, 2)\n",
    "    )(hidden)\n",
    "    \n",
    "    hidden = BatchNormalization(\n",
    "        axis = 3\n",
    "    )(hidden, training = True)\n",
    "    \n",
    "    hidden = Flatten()(hidden)\n",
    "    \n",
    "    hidden = Dense(4096, activation='relu')(hidden)\n",
    "    hidden = Dropout(0.5)(hidden)\n",
    "    hidden = Dense(4096, activation='relu')(hidden)\n",
    "    hidden = Dropout(0.5)(hidden)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax')(hidden)\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'sparse_categorical_crossentropy', \n",
    "        optimizer = optimizers.Adam(learning_rate = learning_rate), \n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063622c-cbdd-4779-aba4-8f40825b5fde",
   "metadata": {},
   "source": [
    "# Utility function to plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795fb01-1125-4f7c-b00b-1b6639409b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_curves(history):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Accuracy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c93ed-9f8a-48d2-bedb-5da770728d95",
   "metadata": {},
   "source": [
    "# Training the model and plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e2d51-c80a-4472-ac94-b13d7ba9665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model()\n",
    "\n",
    "history = model.fit(\n",
    "    x = X_train, y = y_train, \n",
    "    validation_data = (X_test, y_test), \n",
    "    batch_size = 64, epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689758d-c057-43fa-a0c1-c6f87114c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.max(history.history['val_accuracy'])\n",
    "loss = np.min(history.history['val_loss'])\n",
    "\n",
    "print(\"accuracy: \", acc)\n",
    "print(\"loss: \", loss)\n",
    "\n",
    "plot_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58869464-cf3f-47f3-989f-f7579c60a23d",
   "metadata": {},
   "source": [
    "# Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24830429-6258-4723-b98d-e3cde82d90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_name = \"cnn_model_acc=\" + str(np.round(acc, 3)) + \"_loss=\" + str(np.round(acc, 3))\n",
    "model.save(\"saved_models/\" + model_folder_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
